{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface  \n",
    "\n",
    "Most classification algorithms will only perform optimally when the number of samples of each class is roughly the same. Highly skewed datasets, where the minority is heavily outnumbered by one or more classes, have proven to be a challenge while at the same time becoming more and more common.  \n",
    "https://pypi.python.org/pypi/imbalanced-learn \n",
    "\n",
    "Imbalanced data is commonly observed in the real-world setting, real-fraudulent transaction, healthy-infected patient, to name a few. Incautious attempts of machine learning techniques on the problem could give very bad results/prediction. Furthermore, the inappropriate metric of performance measurement provides wrong conclusion due to the nature of the evaluation method. In this project, we try to tackle such problem and compare the model improvement.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Sampling techniques\n",
    "1. Undersampling\n",
    "    - Random undersampling\n",
    "    - Cluster Centroids\n",
    "    - Near Miss\n",
    "2. Oversampling\n",
    "    - Random oversampling: generate new samples by random resampling with replacement of under represented class\n",
    "    - Synthetic Minority Oversampling (SMOTE)\n",
    "3. Combined over and under sampling\n",
    "    - SMOTEENN\n",
    "    - SMOTETomek  \n",
    "\n",
    "### Training techniques  \n",
    "1. Class weighting\n",
    "2. Sample weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss, RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.ensemble import BalanceCascade\n",
    "\n",
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix, \\\n",
    "f1_score, precision_score, auc, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/creditcard.csv')\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df = df.drop(['Time'], axis = 1)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, df.columns != 'Class']\n",
    "y = df.iloc[:, df.columns == 'Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(transformer, X, y):\n",
    "    print('Transforming {}'.format(transformer.__class__.__name__))\n",
    "    X_resampled, y_resampled = transformer.fit_sample(X.values, y.values.ravel())\n",
    "    return transformer.__class__.__name__, pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)\n",
    "\n",
    "def benchmark(sampling_type, X, y):\n",
    "    lr = LogisticRegression(penalty = 'l1')\n",
    "    param_grid = {'C': [0.1,1,10]}\n",
    "    g_search = GridSearchCV(estimator = lr, param_grid = param_grid, scoring = 'accuracy',\n",
    "                            cv = 4, verbose = 2)\n",
    "    g_search = g_search.fit(X.values, y.values.ravel())\n",
    "    return sampling_type, g_search.best_score_, g_search.best_params_['C']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "datasets.append(('base', X_train, y_train))\n",
    "datasets.append(transform(RandomUnderSampler(), X_train, y_train))\n",
    "datasets.append(transform(NearMiss(n_jobs=-1), X_train, y_train))\n",
    "datasets.append(transform(RandomOverSampler(), X_train, y_train))\n",
    "datasets.append(transform(SMOTE(n_jobs=-1), X_train, y_train))\n",
    "\n",
    "## It is computational demanding for larger data sets.\n",
    "## datasets.append(transform(SMOTEENN(), X_train, y_train))\n",
    "## datasets.append(transform(SMOTETomek(), X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display([item[0] for item in datasets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sampling_type, g_search.best_score_, g_search.best_params_['C']\n",
    "benchmark_scores = []\n",
    "for sample_type, X, y in datasets:\n",
    "    print('------')\n",
    "    print('{}'.format(sample_type))\n",
    "    benchmark_scores.append(benchmark(sample_type,X,y))\n",
    "    print('------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(benchmark_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(benchmark_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba\n",
    "\n",
    "## http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "scores = []\n",
    "for sampling_type, score, param in benchmark_scores:\n",
    "    print('Training on {}'.format(sampling_type))\n",
    "    lr = LogisticRegression(penalty='l1', C = param)\n",
    "    for s_type, X, y in datasets:\n",
    "        if s_type == sampling_type:\n",
    "            lr.fit(X.values, y.values.ravel())\n",
    "            y_pred_class = lr.predict(X_test.values)\n",
    "            \n",
    "            ## Probability estimates\n",
    "            y_pred_prob = lr.predict_proba(X_test.values)\n",
    "            \n",
    "            ## Predict confidence scores for samples.\n",
    "            y_pred_confi = lr.decision_function(X_test.values)\n",
    "            \n",
    "            fpr, tpr, threshold = roc_curve(y_test.values.ravel(), y_pred_class)\n",
    "            prec, recall, thres = precision_recall_curve(y_test.values.ravel(), y_pred_confi)\n",
    "\n",
    "            \n",
    "            ## average_precision = average_precision_score(y_test.values.ravel(), y_pred_confi)\n",
    "\n",
    "            \n",
    "            scores.append((sampling_type,\n",
    "                           accuracy_score(y_test.values.ravel(), y_pred_class),\n",
    "                           f1_score(y_test.values.ravel(), y_pred_class),\n",
    "                           precision_score(y_test.values.ravel(), y_pred_class),\n",
    "                           recall_score(y_test.values.ravel(), y_pred_class),\n",
    "                           average_precision_score(y_test.values.ravel(), y_pred_confi),\n",
    "                           auc(fpr, tpr),\n",
    "                           auc(prec, recall, reorder = True),\n",
    "                           confusion_matrix(y_test.values.ravel(), y_pred_class)))  ## tn, fp, fn, tp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_results = pd.DataFrame(scores, columns=['Sampling Type', 'accuracy', 'f1', 'precision',\n",
    "                                                 'recall', 'average_precision', \n",
    "                                                 'auc_roc', 'auc_pr', 'confusion_matrix'])\n",
    "display(sampling_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'l1', class_weight = 'balanced')\n",
    "lr.fit(X_train.values, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "y_pred_class = lr.predict(X_test.values)\n",
    "y_pred_proba = lr.predict_proba(X_test.values)\n",
    "y_pred_confi = lr.decision_function(X_test.values)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.values.ravel(), y_pred_class)\n",
    "precision, recall, thres = precision_recall_curve(y_test.values.ravel(), y_pred_confi)\n",
    "scores.append((\"weighted_base\", \n",
    "               accuracy_score(y_test.values.ravel(),y_pred_class), \n",
    "               f1_score(y_test.values.ravel(),y_pred_class),\n",
    "               precision_score(y_test.values.ravel(),y_pred_class),\n",
    "               recall_score(y_test.values.ravel(),y_pred_class),\n",
    "               average_precision_score(y_test.values.ravel(), y_pred_confi),\n",
    "               auc(fpr, tpr),\n",
    "               auc(precision, recall, reorder=True),\n",
    "               confusion_matrix(y_test.values.ravel(),y_pred_class)))\n",
    "scores = pd.DataFrame(scores, columns = ['Sampling Type', 'accuracy', 'f1', 'precision',\n",
    "                                                 'recall', 'average_precision', \n",
    "                                                 'auc_roc', 'auc_pr', 'confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = sampling_results.append(scores)\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- Undersampling leads to high recall, as long as a huge downside of precision\n",
    "- SMOTE sampling and RandomOverSampler perform the best considering auc_roc and auc_pr with acceptable levels of false positives\n",
    "- Class weighting could give comparable results to sampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
